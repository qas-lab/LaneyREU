{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3390682/1979396960.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['full_text_data']=df['Description']+df['Title']\n",
      "/tmp/ipykernel_3390682/1979396960.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['full_text_data']=df['full_text_data'].fillna('')\n",
      "/tmp/ipykernel_3390682/1979396960.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['isDuplicate'] = df['Duplicated_issue'].apply(lambda x: is_duplicate(x))\n",
      "/home/abarovic/anaconda3/envs/laney_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation target length: 200\n",
      "y_pred length: 200\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "1995    0\n",
      "1996    1\n",
      "1997    0\n",
      "1998    0\n",
      "1999    0\n",
      "Name: isDuplicate, Length: 2000, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.98      0.90       167\n",
      "           1       0.20      0.03      0.05        33\n",
      "\n",
      "    accuracy                           0.82       200\n",
      "   macro avg       0.52      0.50      0.48       200\n",
      "weighted avg       0.73      0.82      0.76       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import tensorflow as t\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords #Word Stop\n",
    "from nltk.tokenize import word_tokenize #Tokenization & Word Stop\"\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "csv_file = '/home/abarovic/laneyREUTest/eclipse_platform.csv'\n",
    "full_df = pd.read_csv(csv_file)\n",
    "\n",
    "full_df.fillna(0,inplace=True)\n",
    "length=len(full_df)\n",
    "split_index=int(.8*length)\n",
    "df=full_df.iloc[:split_index] #df=training set\n",
    "#creating freeform textual data column\n",
    "df['full_text_data']=df['Description']+df['Title']\n",
    "#print(df['full_text_data'])\n",
    "df['full_text_data']=df['full_text_data'].fillna('')\n",
    "def is_duplicate(val):\n",
    "    return 1 if val !=0.0 else 0\n",
    "df['isDuplicate'] = df['Duplicated_issue'].apply(lambda x: is_duplicate(x))\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "X = df['full_text_data']  # Input features\n",
    "y = df['isDuplicate']       # Target variable\n",
    "\n",
    "n = len(df)\n",
    "split_index = int(.8 * n)\n",
    "train_dataset = df.iloc[:split_index]\n",
    "validation_dataset = df.drop(train_dataset.index)\n",
    "\n",
    "train_target = train_dataset['isDuplicate']  # target features\n",
    "#topic_train_priority\n",
    "train_feature = train_dataset['full_text_data']       # input variable\n",
    "#topic_train_data\n",
    "validation_target= validation_dataset['isDuplicate']  # feature features\n",
    "#topic_validation_priority\n",
    "validation_feature= validation_dataset['full_text_data']       # input variable\n",
    "#topic_validation_data \n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['full_text_data'], df['isDuplicate'], test_size=0.25, random_state=42, stratify=df['isDuplicate'])\n",
    "vectorizer = CountVectorizer(stop_words = 'english', tokenizer = word_tokenize, lowercase = True)\n",
    "\n",
    "#train_feature = vectorizer.fit_transform(train_feature)\n",
    "#validation_feature = vectorizer.transform(validation_feature)\n",
    "train_feature = vectorizer.fit_transform(X_train)\n",
    "validation_feature = vectorizer.transform(X_test)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['full_text_data'], df['isDuplicate'], test_size=0.25, random_state=42, stratify=df['isDuplicate'])\n",
    "\n",
    "\n",
    "# Assuming train_feature, train_target, validation_feature, validation_target are already defined\n",
    "\n",
    "# Slice the first 2000 rows for training\n",
    "#train_array = train_feature[:2000].toarray()\n",
    "#train_target = train_target[:2000]\n",
    "train_array = train_feature[:2000].toarray()\n",
    "train_target = train_target[:2000]\n",
    "# Fit the Gaussian Naive Bayes model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train_array, train_target)\n",
    "\n",
    "# Slice the first 200 rows for validation\n",
    "validation_array = validation_feature[:200].toarray()\n",
    "validation_target = validation_target[:200]\n",
    "\n",
    "# Predict using the validation data\n",
    "y_pred = gnb.predict(validation_array)\n",
    "\n",
    "print(\"Validation target length:\", len(validation_target))\n",
    "print(\"y_pred length:\", len(y_pred))\n",
    "print(y_pred)\n",
    "print(train_target)\n",
    "\n",
    "\n",
    "# Assuming train_target and y_pred are already defined\n",
    "# Assuming validation_target and validation_array are already defined from previous slicing\n",
    "\n",
    "# Fit the Gaussian Naive Bayes model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train_array, train_target)\n",
    "\n",
    "# Predict using the validation data\n",
    "y_pred = gnb.predict(validation_array)\n",
    "\n",
    "# Calculate and print the classification report\n",
    "report = classification_report(validation_target, y_pred)\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laney_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
