{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 08:01:11.840177: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-06 08:01:11.843528: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-06 08:01:11.854624: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736150471.873256 3493147 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736150471.878902 3493147 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-06 08:01:11.898772: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/abarovic/anaconda3/envs/laney_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 68124 entries, 0 to 68123\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Issue_id          68124 non-null  int64  \n",
      " 1   Priority          68124 non-null  object \n",
      " 2   Component         68124 non-null  object \n",
      " 3   Duplicated_issue  68124 non-null  float64\n",
      " 4   Title             68124 non-null  object \n",
      " 5   Description       68124 non-null  object \n",
      " 6   Status            68124 non-null  object \n",
      " 7   Resolution        68124 non-null  object \n",
      " 8   Version           68124 non-null  object \n",
      " 9   Created_time      68124 non-null  object \n",
      " 10  Resolved_time     68124 non-null  object \n",
      "dtypes: float64(1), int64(1), object(9)\n",
      "memory usage: 6.2+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3493147/3647427725.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['isDuplicate'] = df['Duplicated_issue'].apply(lambda x: is_duplicate(x))\n",
      "/tmp/ipykernel_3493147/3647427725.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Description'] = df['Description'].astype(str).fillna('')\n",
      "/tmp/ipykernel_3493147/3647427725.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['full_text_data']=df['Description']+df['Title']\n",
      "/tmp/ipykernel_3493147/3647427725.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['full_text_data']=df['full_text_data'].fillna('')\n",
      "/tmp/ipykernel_3493147/3647427725.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['topic'] = topic_distributions.argmax(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue_id</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Component</th>\n",
       "      <th>Duplicated_issue</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Status</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Version</th>\n",
       "      <th>Created_time</th>\n",
       "      <th>Resolved_time</th>\n",
       "      <th>isDuplicate</th>\n",
       "      <th>full_text_data</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>217</td>\n",
       "      <td>P3</td>\n",
       "      <td>Team</td>\n",
       "      <td>0.0</td>\n",
       "      <td>UI BUG TO BE FILED (1GHQOGF)</td>\n",
       "      <td>While minimizing an Eclipse running in Eclipse...</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>INVALID</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2001-10-10 21:38:00 -0400</td>\n",
       "      <td>2001-10-23 23:47:22 -0400</td>\n",
       "      <td>0</td>\n",
       "      <td>While minimizing an Eclipse running in Eclipse...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1533</td>\n",
       "      <td>P2</td>\n",
       "      <td>Debug</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feature: notification of changing variables (1...</td>\n",
       "      <td>DW (12/8/00 4:03:27 PM); \\tThe old VAME debugg...</td>\n",
       "      <td>VERIFIED</td>\n",
       "      <td>FIXED</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2001-10-10 22:14:00 -0400</td>\n",
       "      <td>2002-02-04 13:16:45 -0500</td>\n",
       "      <td>0</td>\n",
       "      <td>DW (12/8/00 4:03:27 PM); \\tThe old VAME debugg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1850</td>\n",
       "      <td>P3</td>\n",
       "      <td>UI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WALKBACK - EditableTable (1G5T6MC)</td>\n",
       "      <td>Debugger Stack Trace Report:; ; Thread[main;5;...</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>FIXED</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2001-10-10 22:20:00 -0400</td>\n",
       "      <td>2001-11-13 15:30:56 -0500</td>\n",
       "      <td>0</td>\n",
       "      <td>Debugger Stack Trace Report:; ; Thread[main;5;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2650</td>\n",
       "      <td>P3</td>\n",
       "      <td>UI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[JFace Text] TextViewer#setVisibleRegion doesn...</td>\n",
       "      <td>Create a TextViewer; give it a string for its ...</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>WONTFIX</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2001-10-10 22:40:00 -0400</td>\n",
       "      <td>2009-08-30 02:31:49 -0400</td>\n",
       "      <td>0</td>\n",
       "      <td>Create a TextViewer; give it a string for its ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2760</td>\n",
       "      <td>P3</td>\n",
       "      <td>UI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Workbench resizing crash (stack overflow) (1GI...</td>\n",
       "      <td>Self hosting Eclipse; my target workbench cras...</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>INVALID</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2001-10-10 22:43:00 -0400</td>\n",
       "      <td>2002-01-08 15:59:57 -0500</td>\n",
       "      <td>0</td>\n",
       "      <td>Self hosting Eclipse; my target workbench cras...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Issue_id Priority Component  Duplicated_issue  \\\n",
       "0       217       P3      Team               0.0   \n",
       "1      1533       P2     Debug               0.0   \n",
       "2      1850       P3        UI               0.0   \n",
       "3      2650       P3        UI               0.0   \n",
       "4      2760       P3        UI               0.0   \n",
       "\n",
       "                                               Title  \\\n",
       "0                       UI BUG TO BE FILED (1GHQOGF)   \n",
       "1  Feature: notification of changing variables (1...   \n",
       "2                 WALKBACK - EditableTable (1G5T6MC)   \n",
       "3  [JFace Text] TextViewer#setVisibleRegion doesn...   \n",
       "4  Workbench resizing crash (stack overflow) (1GI...   \n",
       "\n",
       "                                         Description    Status Resolution  \\\n",
       "0  While minimizing an Eclipse running in Eclipse...  RESOLVED    INVALID   \n",
       "1  DW (12/8/00 4:03:27 PM); \\tThe old VAME debugg...  VERIFIED      FIXED   \n",
       "2  Debugger Stack Trace Report:; ; Thread[main;5;...  RESOLVED      FIXED   \n",
       "3  Create a TextViewer; give it a string for its ...  RESOLVED    WONTFIX   \n",
       "4  Self hosting Eclipse; my target workbench cras...  RESOLVED    INVALID   \n",
       "\n",
       "  Version               Created_time              Resolved_time  isDuplicate  \\\n",
       "0     2.0  2001-10-10 21:38:00 -0400  2001-10-23 23:47:22 -0400            0   \n",
       "1     2.0  2001-10-10 22:14:00 -0400  2002-02-04 13:16:45 -0500            0   \n",
       "2     2.0  2001-10-10 22:20:00 -0400  2001-11-13 15:30:56 -0500            0   \n",
       "3     2.0  2001-10-10 22:40:00 -0400  2009-08-30 02:31:49 -0400            0   \n",
       "4     2.0  2001-10-10 22:43:00 -0400  2002-01-08 15:59:57 -0500            0   \n",
       "\n",
       "                                      full_text_data  topic  \n",
       "0  While minimizing an Eclipse running in Eclipse...      0  \n",
       "1  DW (12/8/00 4:03:27 PM); \\tThe old VAME debugg...      0  \n",
       "2  Debugger Stack Trace Report:; ; Thread[main;5;...      0  \n",
       "3  Create a TextViewer; give it a string for its ...      0  \n",
       "4  Self hosting Eclipse; my target workbench cras...      0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import tensorflow as t\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "csv_file = '/home/abarovic/laneyREUTest/eclipse_platform.csv'\n",
    "full_df = pd.read_csv(csv_file)\n",
    "\n",
    "full_df.fillna(0,inplace=True)\n",
    "df_sorted = full_df.sort_values(by='Created_time')\n",
    "length=len(df_sorted)\n",
    "split_index=int(.8*length)\n",
    "df=df_sorted.iloc[:split_index] #df=training set\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.info()\n",
    "\n",
    "def is_duplicate(val):\n",
    "    return 1 if val !=0.0 else 0\n",
    "df['isDuplicate'] = df['Duplicated_issue'].apply(lambda x: is_duplicate(x))\n",
    "\n",
    "#print(df['isDuplicate'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'isDuplicate' is the column indicating duplicates\n",
    "duplicate_counts = df['isDuplicate'].value_counts()\n",
    "\n",
    "# Check the contents of duplicate_counts\n",
    "#print(duplicate_counts)\n",
    "\n",
    "# Create a list of labels matching the length of duplicate_counts\n",
    "# Ensure the labels match the values in the 'isDuplicate' column\n",
    "labels = duplicate_counts.index.tolist()\n",
    "\n",
    "# Define colors (length should match number of unique values in 'isDuplicate')\n",
    "colors = ['#FFB6C1', '#D8BFD8'][:len(labels)]\n",
    "\n",
    "# Plotting the pie chart\n",
    "#plt.figure(figsize=(8, 6))\n",
    "#plt.pie(duplicate_counts, labels=labels, autopct='%1.1f%%', startangle=140, colors=colors)\n",
    "#plt.title('Percentage of Duplicates vs. Non-Duplicates')\n",
    "#plt.axis('equal')\n",
    "#plt.show()\n",
    "\n",
    "df['Description'] = df['Description'].astype(str).fillna('')\n",
    "\n",
    "#creating freeform textual data column\n",
    "df['full_text_data']=df['Description']+df['Title']\n",
    "#print(df['full_text_data'])\n",
    "df['full_text_data']=df['full_text_data'].fillna('')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')\n",
    "\n",
    "# Fit and transform the CountVectorizer on the text data\n",
    "counts = count_vectorizer.fit_transform(df['full_text_data'])\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')\n",
    "counts = count_vectorizer.fit_transform(df['full_text_data'])\n",
    "\n",
    "        # Fit LDA model\n",
    "lda = LatentDirichletAllocation(n_components=7, max_iter=10,learning_method='online',learning_offset=50.,random_state=0)\n",
    "lda.fit(counts)\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "#for topic_idx, topic in enumerate(lda.components_):\n",
    "#        print(f\"Topic {topic_idx + 1}:\")\n",
    "#        print(\" \".join([feature_names[i]\n",
    "#                        for i in topic.argsort()[:-20 - 1:-1]]))\n",
    "        \n",
    "#        print()\n",
    "\n",
    "######\n",
    "#for alll columns in the df \n",
    "#compare each bug report to each topic \n",
    "#if\n",
    "topic_distributions = lda.transform(counts)\n",
    "\n",
    "df['topic'] = topic_distributions.argmax(axis=1)\n",
    "\n",
    "# Create DataFrames for each topic\n",
    "topic_dfs = []\n",
    "for topic_idx in range(7):\n",
    "    topic_df = df[df['topic'] == topic_idx].copy()\n",
    "    topic_dfs.append(topic_df)\n",
    "\n",
    "# Display the DataFrames for each topic\n",
    "#for i, topic_df in enumerate(topic_dfs):\n",
    "#    print(f\"Topic {i}:\")\n",
    "#    print(topic_df['full_text_data'])\n",
    "#    print()\n",
    "\n",
    "#save each DataFrame to a CSV file\n",
    "for i, topic_df in enumerate(topic_dfs):\n",
    "    topic_df.to_csv(f'topic_{i}_bug_reports.csv', index=False)\n",
    "\n",
    "topic_0_df=pd.read_csv('topic_0_bug_reports.csv')\n",
    "topic_1_df=pd.read_csv('topic_1_bug_reports.csv')\n",
    "topic_2_df=pd.read_csv('topic_2_bug_reports.csv')\n",
    "topic_3_df=pd.read_csv('topic_3_bug_reports.csv')\n",
    "topic_4_df=pd.read_csv('topic_4_bug_reports.csv')\n",
    "topic_5_df=pd.read_csv('topic_5_bug_reports.csv')\n",
    "topic_6_df=pd.read_csv('topic_6_bug_reports.csv')\n",
    "\n",
    "\n",
    "#storaging them in seperate csv so when we do classifciaton on them its alll seperate \n",
    "\n",
    "def precisionRateAtK(df,k):\n",
    "    #updates the row called Experimental_Duplicate_IDs to have the top k most similar bug ids\n",
    "    top_k_recommendations_df=df\n",
    "    matches=find_num_matches(top_k_recommendations_df) #finds relevant results \n",
    "    total=top_k_recommendations_df.shape[0]# Get the number of rows AKA total number of bugs\n",
    "    precisionRateAtK=matches/total\n",
    "    \n",
    "    return precisionRateAtK\n",
    "def recallRateAtK(df,k):\n",
    "    #updates the row called Experimental_Duplicate_IDs to have the top k most similar bug ids\n",
    "    top_k_recommendations_df=get_top_k_similar(df,k)\n",
    "    matches=find_num_matches(top_k_recommendations_df) #finds relevant results \n",
    "    total=top_k_recommendations_df['isDuplicate'].sum()#finds num duplicates\n",
    "    recallRateAtk=matches/total\n",
    "    \n",
    "    return recallRateAtk\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "def get_top_k_similar(df, k, model_name='paraphrase-MiniLM-L6-v2'):\n",
    "    # Load the model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Encode sentences and store the embeddings in the DataFrame\n",
    "    df['embeddings'] = df['full_text_data'].apply(lambda x: model.encode(x, convert_to_tensor=True))\n",
    "    \n",
    "    # Convert embeddings to a tensor\n",
    "    embeddings = torch.stack(df['embeddings'].tolist())\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "    \n",
    "    # Store the top k similar pairs\n",
    "    top_k_similarities = {}\n",
    "    for i in range(len(cosine_scores)):\n",
    "        top_k_similarities[i] = sorted(\n",
    "            [(j, cosine_scores[i][j].item()) for j in range(len(cosine_scores)) if i != j], \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )[:k]\n",
    "    \n",
    "    # Create a new column to store the experimental duplicate Issue IDs\n",
    "    df['Experimental_Duplicate_IDs'] = df.index.map(\n",
    "        lambda idx: [df.loc[pair[0], 'Issue_id'] for pair in top_k_similarities[idx]]\n",
    "    )#this assigns the duplicates based on their ids not the index of the row \n",
    "    return df\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def count_matches(row):\n",
    "    return row['Duplicated_issue'] in row['Experimental_Duplicate_IDs']\n",
    "\n",
    "def find_num_matches(df):\n",
    "    return df.apply(count_matches, axis=1).sum()\n",
    "\n",
    "topic_0_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for topic 0 at 5 is 0.8696808510638298\n",
      "Recall for topic 0 at 5 is 0.13095238095238096\n",
      "Precision for topic 0 at 10 is 0.8696808510638298\n",
      "Recall for topic 0 at 10 is 0.1488095238095238\n",
      "Precision for topic 0 at 15 is 0.8696808510638298\n",
      "Recall for topic 0 at 15 is 0.1488095238095238\n",
      "Precision for topic 0 at 20 is 0.8696808510638298\n",
      "Recall for topic 0 at 20 is 0.16071428571428573\n",
      "Precision for topic 1 at 5 is 0.8414634146341463\n",
      "Recall for topic 1 at 5 is 0.20052770448548812\n",
      "Precision for topic 1 at 10 is 0.8414634146341463\n",
      "Recall for topic 1 at 10 is 0.21635883905013192\n",
      "Precision for topic 1 at 15 is 0.8421807747489239\n",
      "Recall for topic 1 at 15 is 0.23218997361477572\n",
      "Precision for topic 1 at 20 is 0.8421807747489239\n",
      "Recall for topic 1 at 20 is 0.23482849604221637\n",
      "Precision for topic 2 at 5 is 0.8257835922594713\n",
      "Recall for topic 2 at 5 is 0.17920918367346939\n",
      "Precision for topic 2 at 10 is 0.8258381030253475\n",
      "Recall for topic 2 at 10 is 0.21747448979591838\n",
      "Precision for topic 2 at 15 is 0.8258381030253475\n",
      "Recall for topic 2 at 15 is 0.24489795918367346\n",
      "Precision for topic 2 at 20 is 0.8258381030253475\n",
      "Recall for topic 2 at 20 is 0.26498724489795916\n",
      "Precision for topic 3 at 5 is 0.6483825597749648\n",
      "Recall for topic 3 at 5 is 0.07883817427385892\n",
      "Precision for topic 3 at 10 is 0.6483825597749648\n",
      "Recall for topic 3 at 10 is 0.1037344398340249\n",
      "Precision for topic 3 at 15 is 0.6483825597749648\n",
      "Recall for topic 3 at 15 is 0.1078838174273859\n",
      "Precision for topic 3 at 20 is 0.6497890295358649\n",
      "Recall for topic 3 at 20 is 0.11618257261410789\n",
      "Precision for topic 4 at 5 is 0.7155791894222477\n",
      "Recall for topic 4 at 5 is 0.13130765056972327\n",
      "Precision for topic 4 at 10 is 0.7161540672607071\n",
      "Recall for topic 4 at 10 is 0.15572436245252305\n",
      "Precision for topic 4 at 15 is 0.7161540672607071\n",
      "Recall for topic 4 at 15 is 0.16874660879001627\n",
      "Precision for topic 4 at 20 is 0.7161540672607071\n",
      "Recall for topic 4 at 20 is 0.17471513836136734\n",
      "Precision for topic 5 at 5 is 0.833467417538214\n",
      "Recall for topic 5 at 5 is 0.16927083333333334\n",
      "Precision for topic 5 at 10 is 0.833467417538214\n",
      "Recall for topic 5 at 10 is 0.20833333333333334\n",
      "Precision for topic 5 at 15 is 0.833467417538214\n",
      "Recall for topic 5 at 15 is 0.2265625\n",
      "Precision for topic 5 at 20 is 0.833467417538214\n",
      "Recall for topic 5 at 20 is 0.23697916666666666\n",
      "Precision for topic 6 at 5 is 0.8314842069512057\n",
      "Recall for topic 6 at 5 is 0.16931759876859928\n",
      "Precision for topic 6 at 10 is 0.8315408128608627\n",
      "Recall for topic 6 at 10 is 0.20625962031811185\n",
      "Precision for topic 6 at 15 is 0.8315408128608627\n",
      "Recall for topic 6 at 15 is 0.2228493244398837\n",
      "Precision for topic 6 at 20 is 0.8315408128608627\n",
      "Recall for topic 6 at 20 is 0.23807080554130322\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "def get_top_k_similar_v2(df, k, model_name='paraphrase-MiniLM-L6-v2'):\n",
    "    # Load the model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Encode sentences and store the embeddings in the DataFrame\n",
    "    df['embeddings'] = df['full_text_data'].apply(lambda x: model.encode(x, convert_to_tensor=True))\n",
    "    \n",
    "    # Convert embeddings to a tensor\n",
    "    embeddings = torch.stack(df['embeddings'].tolist())\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "    \n",
    "    # Store the top k similar pairs\n",
    "    top_k_similarities = {}\n",
    "    for i in range(len(cosine_scores)):\n",
    "        similarities = [(j, cosine_scores[i][j].item()) for j in range(len(cosine_scores)) if i != j]\n",
    "        top_k_similarities[i] = sorted(similarities, key=lambda x: x[1], reverse=True)[:k]\n",
    "    \n",
    "    # Create a new column to store the experimental duplicate Issue IDs\n",
    "    def get_experimental_duplicates(idx):\n",
    "        similar_pairs = top_k_similarities[idx]\n",
    "        if all(score < 0.95 for _, score in similar_pairs):\n",
    "            return [0]\n",
    "        else:\n",
    "            return [df.loc[pair[0], 'Issue_id'] for pair in similar_pairs]\n",
    "    \n",
    "    df['Experimental_Duplicate_IDs'] = df.index.map(get_experimental_duplicates)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Initialize an empty list to store recall values for each topic\n",
    "all_topic_vals_precision = []\n",
    "all_topic_vals_recall = []\n",
    "\n",
    "for topic_num in range(7):\n",
    "    precision_vals = []\n",
    "    recall_vals = []\n",
    "    \n",
    "    topic_df = globals()[f'topic_{topic_num}_df']  # Dynamically get the DataFrame for the current topic\n",
    "    \n",
    "    # Calculate precision and recall values at different k-values\n",
    "    for k in [5, 10, 15, 20]:\n",
    "        topic_df = get_top_k_similar_v2(topic_df, k)\n",
    "        precision = precisionRateAtK(topic_df, k)\n",
    "        recall = recallRateAtK(topic_df, k)\n",
    "        \n",
    "        precision_vals.append(precision)\n",
    "        recall_vals.append(recall)\n",
    "        \n",
    "        print(f\"Precision for topic {topic_num} at {k} is {precision}\")\n",
    "        print(f\"Recall for topic {topic_num} at {k} is {recall}\")\n",
    "    \n",
    "    # Append precision and recall values for the current topic\n",
    "    all_topic_vals_precision.append(precision_vals)\n",
    "    all_topic_vals_recall.append(recall_vals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Avg Precision Values at Each k:\n",
      "Value 1: 0.7951\n",
      "Value 2: 0.7952\n",
      "Value 3: 0.7953\n",
      "Value 4: 0.7955\n",
      "\n",
      "All Avg Recall Values at Each k:\n",
      "Value 1: 0.1513\n",
      "Value 2: 0.1795\n",
      "Value 3: 0.1931\n",
      "Value 4: 0.2038\n",
      "\n",
      "Summary for Specific k Values:\n",
      "Recall Rate @ 5 is 0.1513\n",
      "Precision Rate @ 5 is 0.7951\n",
      "Recall Rate @ 10 is 0.1795\n",
      "Precision Rate @ 10 is 0.7952\n",
      "Recall Rate @ 15 is 0.1931\n",
      "Precision Rate @ 15 is 0.7953\n",
      "Recall Rate @ 20 is 0.2038\n",
      "Precision Rate @ 20 is 0.7955\n",
      "\n",
      "Mean Average Precision (mAP) is 0.7953\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate averages across all topics\n",
    "average_precision_at_k = np.mean(all_topic_vals_precision, axis=0)\n",
    "average_recall_at_k = np.mean(all_topic_vals_recall, axis=0)\n",
    "mean_average_precision = np.mean(average_precision_at_k)  # Calculate mAP\n",
    "\n",
    "# Print all values in the arrays for precision and recall\n",
    "print(\"All Avg Precision Values at Each k:\")\n",
    "for i, precision_value in enumerate(average_precision_at_k, start=1):\n",
    "    print(f\"Value {i}: {precision_value:.4f}\")\n",
    "\n",
    "print(\"\\nAll Avg Recall Values at Each k:\")\n",
    "for i, recall_value in enumerate(average_recall_at_k, start=1):\n",
    "    print(f\"Value {i}: {recall_value:.4f}\")\n",
    "\n",
    "# Print clean output for specific k values\n",
    "k_values = [5, 10, 15, 20]\n",
    "print(\"\\nSummary for Specific k Values:\")\n",
    "for idx, k in enumerate(k_values):\n",
    "    print(f\"Recall Rate @ {k} is {average_recall_at_k[idx]:.4f}\")\n",
    "    print(f\"Precision Rate @ {k} is {average_precision_at_k[idx]:.4f}\")\n",
    "\n",
    "print(f\"\\nMean Average Precision (mAP) is {mean_average_precision:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laney_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
